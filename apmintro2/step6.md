#Adding a Cache Layer

Right now, our app uses a **sleep** inside the **think** function.

In order to simulate caching, let's add a `@cache.memoize()` function decorator around the **think** function itself.

This will end up storing the value sent to the function, along with the result generated by the function, for 30 seconds at a time.

By adding this cache, we should see a slow first request, followed by fast requests afterwards.

For the instrumentation, we'll use the **Flask-Caching** library, and hook it up to the Redis container we've got running.

Our code, fully instrumented, looks like this:

```python
import requests

from flask import Flask, Response, jsonify
from flask import request as flask_request

from flask_caching import Cache

from ddtrace import tracer, patch
from ddtrace.contrib.flask import TraceMiddleware

from bootstrap import create_app
from models import Thought

from time import sleep

patch(redis=True)
app = create_app()
cache = Cache(config={'CACHE_TYPE': 'redis', 'CACHE_REDIS_HOST': 'redis'})
cache.init_app(app)

traced_app = TraceMiddleware(app, tracer, service='thinker-microservice', distributed_tracing=True)

# Tracer configuration
tracer.configure(hostname='agent')

@tracer.wrap(name='think')
@cache.memoize(30)
def think(subject):
    tracer.current_span().set_tag('subject', subject)

    sleep(0.5)
    quote = Thought.query.filter_by(subject=subject).first()
    return quote

@app.route('/')
def think_microservice():
    # because we have distributed tracing, don't need to manually grab headers
    subject = flask_request.args.get('subject')
    thoughts = think(subject)
    return jsonify(thoughts.serialize())
```

As my favorite side effect of instrumenting, by running some tests through our code again, we can now see how the **Flask-Caching** library implements its caches through Redis:

`curl http://localhost:5000/think/?subject=mankind`{{execute T2}}

`curl http://localhost:5000/think/?subject=mankind`{{execute T2}}

`curl http://localhost:5000/think/?subject=war`{{execute T2}}

Again, running the same cell twice in under 30 seconds should be generated much more quickly than the first request.

And indeed, looking at the APM backend, we can see the changes with a proper cache hit. From 504ms down to 4.63ms:

![cache miss](/technovangelist/scenarios/apmintro2/assets/cache-miss.png)

